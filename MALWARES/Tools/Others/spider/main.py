from urllib.parse import urljoin
import requests
from bs4 import BeautifulSoup

visited_urls = set()

def spider_urls(url, keyword):
    try:
        response = requests.get(url)
    except Exception as e:
        print(f"Request failed {url}: {str(e)}")
        return

    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')
        a_tags = soup.find_all('a')
        urls = []
        for tag in a_tags:
            href = tag.get("href")
            if href is not None and href != "":
                urls.append(href)

        for url2 in urls:
            if url2 not in visited_urls:
                visited_urls.add(url2)
                url_join = urljoin(url, url2)
                if keyword in url_join:
                    print(url_join)
                    spider_urls(url_join, keyword)

if __name__ == "__main__":
    url = input("Enter the URL you want to scrape: ")
    keyword = input("Enter the keyword to search for in the URL provided: ")
    spider_urls(url, keyword)
